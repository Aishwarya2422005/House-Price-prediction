import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression,LogisticRegression
from sklearn.metrics import mean_squared_error, r2_score,accuracy_score

# Load the dataset
data = pd.read_csv('/content/HousePricePrediction.csv')

# Initial exploration
print(data.head())
print(data.info())
print(data.describe())


Data Visualization:

# Check for missing values
print(data.isnull().sum())

# Visualize distributions of features
sns.pairplot(data)
plt.show()




Data Preprocessing:

import pandas as pd

# Assuming 'data' is loaded or assigned correctly
# If 'data' is loaded from a CSV file, for example:
# data = pd.read_csv('your_data.csv')

# Specify columns to delete
columns_to_delete = ['MSZoning', 'LotConfig', 'BldgType', 'Exterior1st']

# Drop the specified columns from the DataFrame
data.drop(columns=columns_to_delete, inplace=True)

# Feature and target variable selection
X = data.drop('SalePrice', axis=1)
y = data['SalePrice']


Splitting the Data:

from sklearn.preprocessing import StandardScaler

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


Building the Model:

from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression

# Initialize SimpleImputer
imputer = SimpleImputer(strategy='mean')

# Impute missing values in the training data
X_train_imputed = imputer.fit_transform(X_train)

target_imputer = SimpleImputer(strategy='mean')

# Impute missing values in the target variable
y_train_imputed = target_imputer.fit_transform(y_train.values.reshape(-1, 1))

# Reshape y_train_imputed back to 1D array
y_train_imputed = y_train_imputed.ravel()

# Initialize and train the model
model = LinearRegression()
model.fit(X_train_imputed, y_train_imputed)


 Model Performance Evaluation

# Initialize SimpleImputer for target variable
target_imputer = SimpleImputer(strategy='mean')

# Impute missing values in the target variable
y_test_imputed = target_imputer.fit_transform(y_test.values.reshape(-1, 1))

# Reshape y_test_imputed back to 1D array
y_test_imputed = y_test_imputed.ravel()

# Compute Mean Absolute Error
mae = mean_absolute_error(y_test_imputed, y_pred)

print(mae)


from sklearn.metrics import mean_squared_error

# Initialize SimpleImputer for target variable
target_imputer = SimpleImputer(strategy='mean')

# Impute missing values in the target variable
y_test_imputed = target_imputer.fit_transform(y_test.values.reshape(-1, 1))

# Reshape y_test_imputed back to 1D array
y_test_imputed = y_test_imputed.ravel()

# Compute Mean Squared Error
mse = mean_squared_error(y_test_imputed, y_pred)

print(mse)
r2 = r2_score(y_test_imputed, y_pred)

r2 = r2_score(y_test_imputed, y_pred)
r2

Conclusion and Insights
Performance Evaluation
Mean Squared Error (MSE): This measures the average squared difference between the predicted values and the actual values. A lower MSE indicates better model performance.
R-squared: This indicates how well the independent variables explain the variability of the dependent variable. An R-squared value closer to 1 indicates a better fit.
